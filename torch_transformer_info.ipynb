{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_transformer_info.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vZci8gm0iEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "In this notebook , i get the transformer example from official documentation.\n",
        "Then i created utilities to print output at every step.\n",
        "I print all tensors with line information and explanation.\n",
        "\"\"\"\n",
        "%matplotlib inline\n",
        "from tabulate import tabulate\n",
        "from IPython.display import HTML as html_print"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm6DdsBM2Ena",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StepLogger():\n",
        "    def __init__(self,capacity):\n",
        "        self.tensor_datas = {}        \n",
        "        self.capacity = capacity\n",
        "        self.added_labels = []\n",
        "        self.comments = []\n",
        "        \n",
        "    \n",
        "    def add_info(self,tensor_data,tensor_label,comment=\"\"):\n",
        "        if tensor_label not in self.added_labels:\n",
        "            self.added_labels.append( tensor_label )\n",
        "            self.comments.append( comment )\n",
        "        \n",
        "        if tensor_label in self.tensor_datas.keys():\n",
        "            current_arr = self.tensor_datas.get(tensor_label)\n",
        "            if len(current_arr) < self.capacity:\n",
        "                current_arr = self.tensor_datas.get(tensor_label, [])\n",
        "                current_arr.append(tensor_data)\n",
        "        else:\n",
        "            self.tensor_datas[tensor_label] = [tensor_data]\n",
        "    \n",
        "    def get_default_summary(self,show_data=False,summary_count=1):\n",
        "        self.get_summary(self.added_labels,show_data,summary_count)        \n",
        "        \n",
        "    def to_html_summary(self,show_data=False,summary_count=1):\n",
        "        print(\"summary_count\",summary_count,\"   self.capacity \",self.capacity)\n",
        "        #table = [['one',''],['',s1],['',\"\"]]\n",
        "        datas = []\n",
        "        count = 0\n",
        "        for i in range(summary_count):\n",
        "            \n",
        "            print(i,\" ------------------------------------------------\")\n",
        "            for l,comment in zip(self.added_labels,self.comments):\n",
        "                \n",
        "                label_data = self.tensor_datas.get(l)[i]\n",
        "                datas.append([\"-----\",\"-----\"])\n",
        "                datas.append([\"\",\"\"])\n",
        "                datas.append([\"\",\"\"])\n",
        "                datas.append([l,\"\"])\n",
        "                datas.append([\"    \"+comment,\"\"])\n",
        "                \n",
        "                if torch.is_tensor(label_data):\n",
        "                    datas.append( [\"\", list(label_data.size() ) ])\n",
        "                if not show_data and not torch.is_tensor(label_data):\n",
        "                    datas.append([\"\",label_data])\n",
        "                if show_data:    \n",
        "                    datas.append([\"\",label_data])\n",
        "                #print(\"subdata\",subdata)    \n",
        "                #datas.append(subdata)\n",
        "        #print(\"datas\",datas)\n",
        "        tabulated = tabulate(datas,headers=['Variable', 'Data'], tablefmt='html')  \n",
        "        #print(\"tabulated\",tabulated)      \n",
        "        to_html = html_print(tabulated)   \n",
        "        #print(\"to_html \",to_html)   \n",
        "        return to_html ,tabulated          \n",
        "          \n",
        "    def get_summary(self,labels,show_data=False,summary_count=1):\n",
        "        print(\"summary_count\",summary_count,\"   self.capacity \",self.capacity)\n",
        "        count = 0\n",
        "        for i in range(summary_count):\n",
        "            print(i,\" ------------------------------------------------\")\n",
        "            for l in labels:\n",
        "                label_data = self.tensor_datas.get(l)[i]\n",
        "                print(l)\n",
        "                if torch.is_tensor(label_data):\n",
        "                    print( list(label_data.size() ) )\n",
        "                if not show_data and not torch.is_tensor(label_data):\n",
        "                    print(label_data)\n",
        "                if show_data:    \n",
        "                    print(label_data)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGiSj330xhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        step_logger.add_info(mask,\"TransformerModel _generate_square_subsequent_mask\",\"mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\")\n",
        "        #[35, 35]\n",
        "        #tensor([[ True, False, False,  ..., False, False, False],\n",
        "        #[ True,  True, False,  ..., False, False, False],\n",
        "        #[ True,  True,  True,  ..., False, False, False],\n",
        "        #...,\n",
        "\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        step_logger.add_info(mask,\"TransformerModel _generate_square_subsequent_mask 2\",\"mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\")\n",
        "        #[35, 35]\n",
        "        #tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
        "        #[0., 0., -inf,  ..., -inf, -inf, -inf],\n",
        "        #[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
        "        #...,\n",
        "        #[0., 0., 0.,  ..., 0., -inf, -inf],\n",
        "        #[0., 0., 0.,  ..., 0., 0., -inf],\n",
        "        #[0., 0., 0.,  ..., 0., 0., 0.]])\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            step_logger.add_info(mask,\"TransformerModel forward mask init \",\"self._generate_square_subsequent_mask(len(src)).to(device)\")\n",
        "            #[35, 35]\n",
        "            #tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
        "            # [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
        "            #[0., 0., 0.,  ..., -inf, -inf, -inf],\n",
        "            #...,\n",
        "            #[0., 0., 0.,  ..., 0., -inf, -inf],\n",
        "            #[0., 0., 0.,  ..., 0., 0., -inf],\n",
        "            #[0., 0., 0.,  ..., 0., 0., 0.]])\n",
        "            self.src_mask = mask\n",
        "            \n",
        "        step_logger.add_info(src,\"TransformerModel forward src\",\"def forward(self, src)\")\n",
        "        #[35, 20]\n",
        "        #tensor([[   14,   148,    36,     6,     8,  1851,   683,    11,     6,   296,\n",
        "        # 25714, 17163,   256,    19,   326,     5,  2670,    22,     4,    22],\n",
        "        #[    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
        "        #     7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4],\n",
        "        #.............\n",
        "        step_logger.add_info(self.ninp,\"TransformerModel forward ninp\")\n",
        "        #200\n",
        "         \n",
        "        #self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        step_logger.add_info(src,\"TransformerModel forward encoder\",\"self.encoder = nn.Embedding(ntoken, ninp)\")\n",
        "        #[35, 20, 200]\n",
        "        # tensor([[[-0.4198, -0.7750, -0.7053,  ..., -0.5248, -0.5563,  1.0408],\n",
        "        # [-0.7180,  0.4915, -0.1442,  ...,  0.7179,  0.6439, -0.5294],\n",
        "        # [-0.6988,  1.1533,  1.3869,  ...,  0.9217,  0.1053,  0.4692],\n",
        "        # ...,\n",
        "        # [ 1.2159,  0.5915, -0.1931,  ..., -0.2329,  0.2632, -0.1759],\n",
        "        # [-0.7472, -0.1905,  0.5676,  ..., -0.3970, -0.4741, -1.0768],\n",
        "        # [ 1.2159,  0.5915, -0.1931,  ..., -0.2329,  0.2632, -0.1759]],\n",
        "\n",
        "\n",
        "        src = self.pos_encoder(src)\n",
        "        step_logger.add_info(src,\"TransformerModel forward pos_encoder\",\"src = self.pos_encoder(src)\")\n",
        "        #[35, 20, 200]\n",
        "        #tensor([[[-0.0000e+00,  2.8131e-01, -8.8157e-01,  ...,  5.9396e-01,\n",
        "        #  -6.9536e-01,  0.0000e+00],\n",
        "\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        step_logger.add_info(output,\"TransformerModel transformer_encoder output\",\"output = self.transformer_encoder(src, self.src_mask)\")\n",
        "        #[35, 20, 200]\n",
        "        #tensor([[[-8.9633e-01, -1.2172e+00, -6.3429e-01,  ...,  1.9572e-01,\n",
        "        #  -7.4439e-01,  8.3845e-01],\n",
        "\n",
        "        #self.decoder = nn.Linear(ninp, ntoken)\n",
        "        output = self.decoder(output)\n",
        "        step_logger.add_info(output,\"TransformerModel decoder output\",\"output = self.decoder(output)\")\n",
        "        #[35, 20, 28871]\n",
        "        #tensor([[[-8.1877e-02,  3.2367e-01,  4.9415e-01,  ..., -1.6005e+00,\n",
        "        #   7.6720e-01,  8.4653e-01],\n",
        "\n",
        "        return output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAONeq5q00EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        \n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        step_logger.add_info(div_term,\"PositionalEncoding  div_term\",\"div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\")\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        step_logger.add_info(pe,\"PositionalEncoding  pe\")\n",
        "\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        step_logger.add_info(pe,\"PositionalEncoding  pe unsqueeze\",\"pe = pe.unsqueeze(0).transpose(0, 1)\")\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        step_logger.add_info(x,\"PositionalEncoding  forward\",\"def forward(self, x):\")\n",
        "        #[35, 20, 200]\n",
        "        #tensor([[[-0.4198, -0.7750, -0.7053,  ..., -0.5248, -0.5563,  1.0408],\n",
        "        # [-0.7180,  0.4915, -0.1442,  ...,  0.7179,  0.6439, -0.5294],\n",
        "        # [-0.6988,  1.1533,  1.3869,  ...,  0.9217,  0.1053,  0.4692],\n",
        "        # ...,\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        step_logger.add_info(x,\"PositionalEncoding  forward self.pe\",\"x = x + self.pe[:x.size(0), :]\")\n",
        "        #[35, 20, 200]\n",
        "        #tensor([[[-4.1976e-01,  2.2504e-01, -7.0526e-01,  ...,  4.7516e-01,\n",
        "        #  -5.5629e-01,  2.0408e+00],\n",
        "\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD10f9xo01sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7dQl5gHpXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "60214fb7-faa5-4d52-b735-397966b8fb41"
      },
      "source": [
        "\n",
        "print( train_txt.examples[0].__dict__[\"text\"][30:50] )\n",
        "print( len(train_txt.examples[0].__dict__[\"text\"] ))\n",
        "\n",
        "print( test_txt.examples[0].__dict__[\"text\"][30:50] )\n",
        "print( len(test_txt.examples[0].__dict__[\"text\"] ))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@']\n",
            "2236652\n",
            "['guest', '@-@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.', 'this', 'was', 'followed', 'by', 'a', 'starring', 'role']\n",
            "280576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzTlTM-r04b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    \n",
        "    #print(\"data\",i,i+seq_len) #data 0 35\n",
        "    #print(\"target\",i+1,i+1+seq_len) #target 1 36\n",
        "\n",
        "    #print(\"source\",source.size() ) #source torch.Size([111832, 20])\n",
        "\n",
        "\n",
        "    data = source[i:i+seq_len]\n",
        "    #print(\"data\",data.size() ) #data torch.Size([35, 20])\n",
        "\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    #print(\"target\",target.size() ) #target torch.Size([700])\n",
        "\n",
        "    return data, target"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI-3EVWm6OfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c90748d-8bef-485d-e3cd-0c5317aea79f"
      },
      "source": [
        "data, targets = get_batch(train_data, 0)\n",
        "targets.size()\n",
        "#data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([700])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQs41uV706wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "step_logger = StepLogger(2)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU8vg_9Z7vSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "464e0e6e-91e9-469d-d739-a52374541b65"
      },
      "source": [
        "print(train_data.size() )\n",
        "print(train_data[0] )\n",
        "#for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "#  print(batch,i)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([111832, 20])\n",
            "tensor([   14,   148,    36,     6,     8,  1851,   683,    11,     6,   296,\n",
            "        25714, 17163,   256,    19,   326,     5,  2670,    22,     4,    22],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZVBoDXu08YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi) #28871\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    #for batch, i in enumerate(range(0, 300, bptt)):  \n",
        "        data, targets = get_batch(train_data, i)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        step_logger.add_info(data,\"train data\")\n",
        "        #[35, 20]\n",
        "        #tensor([[   14,   148,    36,     6,     8,  1851,   683,    11,     6,   296,\n",
        "        # 25714, 17163,   256,    19,   326,     5,  2670,    22,     4,    22],\n",
        "        #[    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
        "        #     7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4],\n",
        "        #.........................................\n",
        "        \n",
        "        #target is 1 step after train, predicting next sequence\n",
        "        step_logger.add_info(targets,\"train targets\")\n",
        "        #[700]\n",
        "        #[    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
        "        #    7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4,\n",
        "        #   14,   674,     7,    14,     9,     6,  2642,   483,  9818,    41,\n",
        "        #   .............................\n",
        "\n",
        "        output = model(data)\n",
        "        step_logger.add_info(output,\"train output\")\n",
        "        #[35, 20, 28871]\n",
        "        #tensor([[[-8.1877e-02,  3.2367e-01,  4.9415e-01,  ..., -1.6005e+00,\n",
        "        #   7.6720e-01,  8.4653e-01],\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69igHcxJ0-nt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "664bb35e-e2b4-45cb-e4ea-10a1fca584c7"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 1 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 3195 batches | lr 5.00 | ms/batch 17.22 | loss  7.32 | ppl  1511.88\n",
            "| epoch   1 |   400/ 3195 batches | lr 5.00 | ms/batch 16.91 | loss  6.27 | ppl   528.88\n",
            "| epoch   1 |   600/ 3195 batches | lr 5.00 | ms/batch 16.96 | loss  5.97 | ppl   391.32\n",
            "| epoch   1 |   800/ 3195 batches | lr 5.00 | ms/batch 17.00 | loss  5.80 | ppl   330.58\n",
            "| epoch   1 |  1000/ 3195 batches | lr 5.00 | ms/batch 17.07 | loss  5.81 | ppl   332.22\n",
            "| epoch   1 |  1200/ 3195 batches | lr 5.00 | ms/batch 17.14 | loss  5.75 | ppl   313.47\n",
            "| epoch   1 |  1400/ 3195 batches | lr 5.00 | ms/batch 17.17 | loss  5.72 | ppl   303.55\n",
            "| epoch   1 |  1600/ 3195 batches | lr 5.00 | ms/batch 17.25 | loss  5.61 | ppl   272.41\n",
            "| epoch   1 |  1800/ 3195 batches | lr 5.00 | ms/batch 17.37 | loss  5.63 | ppl   278.93\n",
            "| epoch   1 |  2000/ 3195 batches | lr 5.00 | ms/batch 17.40 | loss  5.63 | ppl   279.25\n",
            "| epoch   1 |  2200/ 3195 batches | lr 5.00 | ms/batch 17.45 | loss  5.57 | ppl   261.33\n",
            "| epoch   1 |  2400/ 3195 batches | lr 5.00 | ms/batch 17.49 | loss  5.46 | ppl   234.30\n",
            "| epoch   1 |  2600/ 3195 batches | lr 5.00 | ms/batch 17.63 | loss  5.53 | ppl   252.55\n",
            "| epoch   1 |  2800/ 3195 batches | lr 5.00 | ms/batch 17.67 | loss  5.51 | ppl   247.71\n",
            "| epoch   1 |  3000/ 3195 batches | lr 5.00 | ms/batch 17.75 | loss  5.42 | ppl   225.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 58.25s | valid loss  5.15 | valid ppl   171.90\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrIk0xU91DO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iht4lhk4yP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "0e91711c-5925-4d65-e194-1d3dbe9e1c21"
      },
      "source": [
        "step_logger.get_default_summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "summary_count 1    self.capacity  2\n",
            "0  ------------------------------------------------\n",
            "PositionalEncoding  div_term\n",
            "[100]\n",
            "PositionalEncoding  pe\n",
            "[5000, 200]\n",
            "PositionalEncoding  pe unsqueeze\n",
            "[5000, 1, 200]\n",
            "train data\n",
            "[35, 20]\n",
            "train targets\n",
            "[700]\n",
            "TransformerModel _generate_square_subsequent_mask\n",
            "[35, 35]\n",
            "TransformerModel _generate_square_subsequent_mask 2\n",
            "[35, 35]\n",
            "TransformerModel forward mask init \n",
            "[35, 35]\n",
            "TransformerModel forward src\n",
            "[35, 20]\n",
            "TransformerModel forward ninp\n",
            "200\n",
            "TransformerModel forward encoder\n",
            "[35, 20, 200]\n",
            "PositionalEncoding  forward\n",
            "[35, 20, 200]\n",
            "PositionalEncoding  forward self.pe\n",
            "[35, 20, 200]\n",
            "TransformerModel forward pos_encoder\n",
            "[35, 20, 200]\n",
            "TransformerModel transformer_encoder output\n",
            "[35, 20, 200]\n",
            "TransformerModel decoder output\n",
            "[35, 20, 28871]\n",
            "train output\n",
            "[35, 20, 28871]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbSN2HSzh2wp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6698f5f9-5392-4653-b1bd-1123434c04e6"
      },
      "source": [
        "h = step_logger.to_html_summary(True)\n",
        "display(h[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "summary_count 1    self.capacity  2\n",
            "0  ------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>Variable                                                                                    </th><th>Data           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>PositionalEncoding  div_term                                                                </td><td>               </td></tr>\n",
              "<tr><td>div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[100]          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([1.0000e+00, 9.1201e-01, 8.3176e-01, 7.5858e-01, 6.9183e-01, 6.3096e-01,\n",
              "        5.7544e-01, 5.2481e-01, 4.7863e-01, 4.3652e-01, 3.9811e-01, 3.6308e-01,\n",
              "        3.3113e-01, 3.0200e-01, 2.7542e-01, 2.5119e-01, 2.2909e-01, 2.0893e-01,\n",
              "        1.9055e-01, 1.7378e-01, 1.5849e-01, 1.4454e-01, 1.3183e-01, 1.2023e-01,\n",
              "        1.0965e-01, 1.0000e-01, 9.1201e-02, 8.3176e-02, 7.5858e-02, 6.9183e-02,\n",
              "        6.3096e-02, 5.7544e-02, 5.2481e-02, 4.7863e-02, 4.3652e-02, 3.9811e-02,\n",
              "        3.6308e-02, 3.3113e-02, 3.0200e-02, 2.7542e-02, 2.5119e-02, 2.2909e-02,\n",
              "        2.0893e-02, 1.9055e-02, 1.7378e-02, 1.5849e-02, 1.4454e-02, 1.3183e-02,\n",
              "        1.2023e-02, 1.0965e-02, 1.0000e-02, 9.1201e-03, 8.3176e-03, 7.5858e-03,\n",
              "        6.9183e-03, 6.3096e-03, 5.7544e-03, 5.2481e-03, 4.7863e-03, 4.3652e-03,\n",
              "        3.9811e-03, 3.6308e-03, 3.3113e-03, 3.0200e-03, 2.7542e-03, 2.5119e-03,\n",
              "        2.2909e-03, 2.0893e-03, 1.9055e-03, 1.7378e-03, 1.5849e-03, 1.4454e-03,\n",
              "        1.3183e-03, 1.2023e-03, 1.0965e-03, 1.0000e-03, 9.1201e-04, 8.3176e-04,\n",
              "        7.5858e-04, 6.9183e-04, 6.3096e-04, 5.7544e-04, 5.2481e-04, 4.7863e-04,\n",
              "        4.3652e-04, 3.9811e-04, 3.6308e-04, 3.3113e-04, 3.0200e-04, 2.7542e-04,\n",
              "        2.5119e-04, 2.2909e-04, 2.0893e-04, 1.9055e-04, 1.7378e-04, 1.5849e-04,\n",
              "        1.4454e-04, 1.3183e-04, 1.2023e-04, 1.0965e-04])                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>PositionalEncoding  pe                                                                      </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[5000, 200]    </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "          0.0000e+00,  1.0000e+00],\n",
              "        [ 8.4147e-01,  5.4030e-01,  7.9074e-01,  ...,  1.0000e+00,\n",
              "          1.0965e-04,  1.0000e+00],\n",
              "        [ 9.0930e-01, -4.1615e-01,  9.6811e-01,  ...,  1.0000e+00,\n",
              "          2.1930e-04,  1.0000e+00],\n",
              "        ...,\n",
              "        [ 9.5625e-01, -2.9254e-01,  9.0551e-01,  ...,  8.2490e-01,\n",
              "          5.2090e-01,  8.5362e-01],\n",
              "        [ 2.7050e-01, -9.6272e-01,  2.1917e-01,  ...,  8.2483e-01,\n",
              "          5.2100e-01,  8.5356e-01],\n",
              "        [-6.6395e-01, -7.4778e-01, -6.3742e-01,  ...,  8.2476e-01,\n",
              "          5.2109e-01,  8.5350e-01]])                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>PositionalEncoding  pe unsqueeze                                                            </td><td>               </td></tr>\n",
              "<tr><td>pe = pe.unsqueeze(0).transpose(0, 1)                                                        </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[5000, 1, 200] </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "           0.0000e+00,  1.0000e+00]],\n",
              "\n",
              "        [[ 8.4147e-01,  5.4030e-01,  7.9074e-01,  ...,  1.0000e+00,\n",
              "           1.0965e-04,  1.0000e+00]],\n",
              "\n",
              "        [[ 9.0930e-01, -4.1615e-01,  9.6811e-01,  ...,  1.0000e+00,\n",
              "           2.1930e-04,  1.0000e+00]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 9.5625e-01, -2.9254e-01,  9.0551e-01,  ...,  8.2490e-01,\n",
              "           5.2090e-01,  8.5362e-01]],\n",
              "\n",
              "        [[ 2.7050e-01, -9.6272e-01,  2.1917e-01,  ...,  8.2483e-01,\n",
              "           5.2100e-01,  8.5356e-01]],\n",
              "\n",
              "        [[-6.6395e-01, -7.4778e-01, -6.3742e-01,  ...,  8.2476e-01,\n",
              "           5.2109e-01,  8.5350e-01]]])                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>train data                                                                                  </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20]       </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[   14,   148,    36,     6,     8,  1851,   683,    11,     6,   296,\n",
              "         25714, 17163,   256,    19,   326,     5,  2670,    22,     4,    22],\n",
              "        [    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
              "             7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4],\n",
              "        [   14,   674,     7,    14,     9,     6,  2642,   483,  9818,    41,\n",
              "           598,   338,     8, 13145,    15,   611,    28,  1463,    78,   375],\n",
              "        [   16,   158,   302,    20,    28,    26,   628,    27,    25,   476,\n",
              "             5,    15,    10, 10155, 13908,    83, 14646,    13,    37,    18],\n",
              "        [ 3876,   550,  1943, 23160,   554,     4,    11,   659,    15,     6,\n",
              "            62,   242,     9,  1870,   666,    22, 16025,   659,  1395,     4],\n",
              "        [ 3896,    13,    31, 22229,     8,   776,  2875,  1490,  2898,     3,\n",
              "             4,   249,    22,  1079,    19,    41,    27,    21,    13,   883],\n",
              "        [  890,     4,  5694,     5,    10,     7,   129, 12028,     7,    14,\n",
              "           299,    20,     4,    13,    95,   447,    11,  2087, 25106,   251],\n",
              "        [   16, 12285,     6,  1560,     9,   192,    23,    25,    81,     4,\n",
              "             7,     4,  1094,     8,  2820,   513,   752,  9848,     4,     5],\n",
              "        [    3,     6,     4,  5144,    27,     5,  2695,   343,    48, 19751,\n",
              "           598,  3396,    18,    10,  1312,    39, 19410,    11,   710,     4],\n",
              "        [   14,   674,   342,  1417,     5,     4,   442,  2807,    99,   403,\n",
              "            37,   796,  1188,     9,   152,  1669,    28, 10885,    21,   918],\n",
              "        [    3,   214,     7,    13,     8,   122,    25,     5,  1023,    26,\n",
              "          1114,     5,    26,     8,   150,    13,   181,   281,   744,    15],\n",
              "        [   14,    15,  4038,   668,    10,  3290,  1685,    15,    28,     4,\n",
              "            13,     4,   946,    10,     8,  7111, 10522,  2224,     6,   883],\n",
              "        [20061,    82,    18,    70,     9,   759,  1868,   646,   179,  4431,\n",
              "          6708,   550,     8,     9,    10,    19,    27,    75,  4968,   765],\n",
              "        [   94,    19,  1627,  2029,     5,  3110,   907,    19,    61,     7,\n",
              "          1677,     7,    10,    11,     9,   732,   149,    69,  2627,    11],\n",
              "        [ 3876,  1107,     8,  1462,     8,    12,    12,     8,   197,  8042,\n",
              "          5711,     8,     9,    15,    11,  1375, 13268,     5,    13,   111],\n",
              "        [   99,  1551,    10,    13,    10,     4,     4,    10,    48,   159,\n",
              "         19679,    10, 18736,    49,     8,    45,    12,   383,    17,    35],\n",
              "        [   53, 10674,     9,  4302,     9,   157,   356,     9,    74,    22,\n",
              "            12,     9,     7,    19,    10,    41,    36,    13,   293,  7943],\n",
              "        [    8,    31,    26,    24,     5,    35,     6,   301,    61,  8278,\n",
              "             4,  3396,     8,   686,     9,   737,   695,     4,    15,     6],\n",
              "        [   10,    33,  6443,    15,     8,  2686,     3,   440,    81,    23,\n",
              "           108,     5,    10, 10155,     5,    18,    13,  1308,     8,  1448],\n",
              "        [    9,  8440,    12,   323,    10,   397,    14,  5328,    12,   473,\n",
              "             6,    38,     9,   347,    44,  1142,  4145,    20,    10,   158],\n",
              "        [ 3896,     5,  6129,     6,     9,    13,     4,    26,    27,    18,\n",
              "         25714,  3625,    28,     6,    63,    93,   195,     4,     9,    39],\n",
              "        [   28,    33,    56,  1417,     5,    94,  6125,     4,     5,    31,\n",
              "             5,    32,    12,     4,  1544,    87,    29,  2394,   866,  5680],\n",
              "        [  790,  2725,  9474,    35,     8,   771,  5315,   910,     4,    17,\n",
              "            54,     4,     4,  4483,  1009,  2462,  3257,    11,    17,    85],\n",
              "        [   53,     5,     6,   851,    10,    11,    13,    24,  3664, 28803,\n",
              "          4185,   455,   450,   231,    12,    11,    25,  7081,   189,    33],\n",
              "        [28869,     4,    12,    13,     9,  3505,   347,   948, 12089,    17,\n",
              "          6438,  3693,     8,    15,  1637,  1287,  3680,     6,   184,  2261],\n",
              "        [    5,  1560,  9091,  1411,     5,  2168,    93,     5,  9115,     5,\n",
              "             5,   183,    10,   522,    72,     6,     5,    85,   572,  2660],\n",
              "        [ 6217,    11,     5,    42,     8,     5,     4,   768,    31,    15,\n",
              "          1927,     5,     9,    48,  1075,     3,    62,   179,    35,    11],\n",
              "        [    6,     4,    30,   705,    10,   441,  4073,    11,  8933,   283,\n",
              "             4,    25,  1160,   648,     7,    14,    29,    61,    13,   319],\n",
              "        [ 3876,  2333,   135,     7,     9, 11361,    11,  1936,  9829,    38,\n",
              "           649,     8,    27,   423,  6996,     3,  6665,   352,  2137,   231],\n",
              "        [    7,     7,     4,   211,     5,    20,   231,    45,  6214,   158,\n",
              "          1057,    10,    12,    12,   189,    14,    26,   187,   405, 14028],\n",
              "        [    4,   517,   362,  6168,     8,  1022,    15,   327,  3112,  3929,\n",
              "             7,     9,     4,     4,     4,    16,    33,     7,     7,  1993],\n",
              "        [ 5054,    66,  2429,    56,    10,  3128,   517,  1519,   220,    13,\n",
              "             4,  8857,  1891,   564,  2120,    16,   289,   281,     4,  3829],\n",
              "        [   99, 16509,     7,  3476,     9,  1957,    48,    58,     4,   599,\n",
              "         11019,    21,   141,    20,    18,    16,     8,  4158,   789,     6],\n",
              "        [   27,  1072,  4038, 10060,     5,    11,   169,    15, 11115, 12527,\n",
              "             7,  3396,     6,    15,    47,  1415,    10,   166,   332,    68],\n",
              "        [    5,     6,   362,    22,  7991,  7995,   423,  3207,     6,     5,\n",
              "             4,     6,   116,   419,  2843,  3435,     9,    31,     7,     8]],\n",
              "       device=&#x27;cuda:0&#x27;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>train targets                                                                               </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[700]          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
              "            7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4,\n",
              "           14,   674,     7,    14,     9,     6,  2642,   483,  9818,    41,\n",
              "          598,   338,     8, 13145,    15,   611,    28,  1463,    78,   375,\n",
              "           16,   158,   302,    20,    28,    26,   628,    27,    25,   476,\n",
              "            5,    15,    10, 10155, 13908,    83, 14646,    13,    37,    18,\n",
              "         3876,   550,  1943, 23160,   554,     4,    11,   659,    15,     6,\n",
              "           62,   242,     9,  1870,   666,    22, 16025,   659,  1395,     4,\n",
              "         3896,    13,    31, 22229,     8,   776,  2875,  1490,  2898,     3,\n",
              "            4,   249,    22,  1079,    19,    41,    27,    21,    13,   883,\n",
              "          890,     4,  5694,     5,    10,     7,   129, 12028,     7,    14,\n",
              "          299,    20,     4,    13,    95,   447,    11,  2087, 25106,   251,\n",
              "           16, 12285,     6,  1560,     9,   192,    23,    25,    81,     4,\n",
              "            7,     4,  1094,     8,  2820,   513,   752,  9848,     4,     5,\n",
              "            3,     6,     4,  5144,    27,     5,  2695,   343,    48, 19751,\n",
              "          598,  3396,    18,    10,  1312,    39, 19410,    11,   710,     4,\n",
              "           14,   674,   342,  1417,     5,     4,   442,  2807,    99,   403,\n",
              "           37,   796,  1188,     9,   152,  1669,    28, 10885,    21,   918,\n",
              "            3,   214,     7,    13,     8,   122,    25,     5,  1023,    26,\n",
              "         1114,     5,    26,     8,   150,    13,   181,   281,   744,    15,\n",
              "           14,    15,  4038,   668,    10,  3290,  1685,    15,    28,     4,\n",
              "           13,     4,   946,    10,     8,  7111, 10522,  2224,     6,   883,\n",
              "        20061,    82,    18,    70,     9,   759,  1868,   646,   179,  4431,\n",
              "         6708,   550,     8,     9,    10,    19,    27,    75,  4968,   765,\n",
              "           94,    19,  1627,  2029,     5,  3110,   907,    19,    61,     7,\n",
              "         1677,     7,    10,    11,     9,   732,   149,    69,  2627,    11,\n",
              "         3876,  1107,     8,  1462,     8,    12,    12,     8,   197,  8042,\n",
              "         5711,     8,     9,    15,    11,  1375, 13268,     5,    13,   111,\n",
              "           99,  1551,    10,    13,    10,     4,     4,    10,    48,   159,\n",
              "        19679,    10, 18736,    49,     8,    45,    12,   383,    17,    35,\n",
              "           53, 10674,     9,  4302,     9,   157,   356,     9,    74,    22,\n",
              "           12,     9,     7,    19,    10,    41,    36,    13,   293,  7943,\n",
              "            8,    31,    26,    24,     5,    35,     6,   301,    61,  8278,\n",
              "            4,  3396,     8,   686,     9,   737,   695,     4,    15,     6,\n",
              "           10,    33,  6443,    15,     8,  2686,     3,   440,    81,    23,\n",
              "          108,     5,    10, 10155,     5,    18,    13,  1308,     8,  1448,\n",
              "            9,  8440,    12,   323,    10,   397,    14,  5328,    12,   473,\n",
              "            6,    38,     9,   347,    44,  1142,  4145,    20,    10,   158,\n",
              "         3896,     5,  6129,     6,     9,    13,     4,    26,    27,    18,\n",
              "        25714,  3625,    28,     6,    63,    93,   195,     4,     9,    39,\n",
              "           28,    33,    56,  1417,     5,    94,  6125,     4,     5,    31,\n",
              "            5,    32,    12,     4,  1544,    87,    29,  2394,   866,  5680,\n",
              "          790,  2725,  9474,    35,     8,   771,  5315,   910,     4,    17,\n",
              "           54,     4,     4,  4483,  1009,  2462,  3257,    11,    17,    85,\n",
              "           53,     5,     6,   851,    10,    11,    13,    24,  3664, 28803,\n",
              "         4185,   455,   450,   231,    12,    11,    25,  7081,   189,    33,\n",
              "        28869,     4,    12,    13,     9,  3505,   347,   948, 12089,    17,\n",
              "         6438,  3693,     8,    15,  1637,  1287,  3680,     6,   184,  2261,\n",
              "            5,  1560,  9091,  1411,     5,  2168,    93,     5,  9115,     5,\n",
              "            5,   183,    10,   522,    72,     6,     5,    85,   572,  2660,\n",
              "         6217,    11,     5,    42,     8,     5,     4,   768,    31,    15,\n",
              "         1927,     5,     9,    48,  1075,     3,    62,   179,    35,    11,\n",
              "            6,     4,    30,   705,    10,   441,  4073,    11,  8933,   283,\n",
              "            4,    25,  1160,   648,     7,    14,    29,    61,    13,   319,\n",
              "         3876,  2333,   135,     7,     9, 11361,    11,  1936,  9829,    38,\n",
              "          649,     8,    27,   423,  6996,     3,  6665,   352,  2137,   231,\n",
              "            7,     7,     4,   211,     5,    20,   231,    45,  6214,   158,\n",
              "         1057,    10,    12,    12,   189,    14,    26,   187,   405, 14028,\n",
              "            4,   517,   362,  6168,     8,  1022,    15,   327,  3112,  3929,\n",
              "            7,     9,     4,     4,     4,    16,    33,     7,     7,  1993,\n",
              "         5054,    66,  2429,    56,    10,  3128,   517,  1519,   220,    13,\n",
              "            4,  8857,  1891,   564,  2120,    16,   289,   281,     4,  3829,\n",
              "           99, 16509,     7,  3476,     9,  1957,    48,    58,     4,   599,\n",
              "        11019,    21,   141,    20,    18,    16,     8,  4158,   789,     6,\n",
              "           27,  1072,  4038, 10060,     5,    11,   169,    15, 11115, 12527,\n",
              "            7,  3396,     6,    15,    47,  1415,    10,   166,   332,    68,\n",
              "            5,     6,   362,    22,  7991,  7995,   423,  3207,     6,     5,\n",
              "            4,     6,   116,   419,  2843,  3435,     9,    31,     7,     8,\n",
              "         1847,     3,     5,   158,     8,    24,    71,    13,  6276, 19321,\n",
              "        11176,     3,     5,    19,    84,  3206,  5253,     4,  2390,    10],\n",
              "       device=&#x27;cuda:0&#x27;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel _generate_square_subsequent_mask                                           </td><td>               </td></tr>\n",
              "<tr><td>mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)                                </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 35]       </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[ True, False, False,  ..., False, False, False],\n",
              "        [ True,  True, False,  ..., False, False, False],\n",
              "        [ True,  True,  True,  ..., False, False, False],\n",
              "        ...,\n",
              "        [ True,  True,  True,  ...,  True, False, False],\n",
              "        [ True,  True,  True,  ...,  True,  True, False],\n",
              "        [ True,  True,  True,  ...,  True,  True,  True]])                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel _generate_square_subsequent_mask 2                                         </td><td>               </td></tr>\n",
              "<tr><td>mask = mask.float().masked_fill(mask == 0, float(&#x27;-inf&#x27;)).masked_fill(mask == 1, float(0.0))</td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 35]       </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel forward mask init                                                          </td><td>               </td></tr>\n",
              "<tr><td>self._generate_square_subsequent_mask(len(src)).to(device)                                  </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 35]       </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device=&#x27;cuda:0&#x27;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel forward src                                                                </td><td>               </td></tr>\n",
              "<tr><td>def forward(self, src)                                                                      </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20]       </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[   14,   148,    36,     6,     8,  1851,   683,    11,     6,   296,\n",
              "         25714, 17163,   256,    19,   326,     5,  2670,    22,     4,    22],\n",
              "        [    3,    22, 10588,     3,    10,   973,    18,  2069,  2165,     7,\n",
              "             7,    37,  9283,   686,    12,  3692,  5020,    40,  3188,     4],\n",
              "        [   14,   674,     7,    14,     9,     6,  2642,   483,  9818,    41,\n",
              "           598,   338,     8, 13145,    15,   611,    28,  1463,    78,   375],\n",
              "        [   16,   158,   302,    20,    28,    26,   628,    27,    25,   476,\n",
              "             5,    15,    10, 10155, 13908,    83, 14646,    13,    37,    18],\n",
              "        [ 3876,   550,  1943, 23160,   554,     4,    11,   659,    15,     6,\n",
              "            62,   242,     9,  1870,   666,    22, 16025,   659,  1395,     4],\n",
              "        [ 3896,    13,    31, 22229,     8,   776,  2875,  1490,  2898,     3,\n",
              "             4,   249,    22,  1079,    19,    41,    27,    21,    13,   883],\n",
              "        [  890,     4,  5694,     5,    10,     7,   129, 12028,     7,    14,\n",
              "           299,    20,     4,    13,    95,   447,    11,  2087, 25106,   251],\n",
              "        [   16, 12285,     6,  1560,     9,   192,    23,    25,    81,     4,\n",
              "             7,     4,  1094,     8,  2820,   513,   752,  9848,     4,     5],\n",
              "        [    3,     6,     4,  5144,    27,     5,  2695,   343,    48, 19751,\n",
              "           598,  3396,    18,    10,  1312,    39, 19410,    11,   710,     4],\n",
              "        [   14,   674,   342,  1417,     5,     4,   442,  2807,    99,   403,\n",
              "            37,   796,  1188,     9,   152,  1669,    28, 10885,    21,   918],\n",
              "        [    3,   214,     7,    13,     8,   122,    25,     5,  1023,    26,\n",
              "          1114,     5,    26,     8,   150,    13,   181,   281,   744,    15],\n",
              "        [   14,    15,  4038,   668,    10,  3290,  1685,    15,    28,     4,\n",
              "            13,     4,   946,    10,     8,  7111, 10522,  2224,     6,   883],\n",
              "        [20061,    82,    18,    70,     9,   759,  1868,   646,   179,  4431,\n",
              "          6708,   550,     8,     9,    10,    19,    27,    75,  4968,   765],\n",
              "        [   94,    19,  1627,  2029,     5,  3110,   907,    19,    61,     7,\n",
              "          1677,     7,    10,    11,     9,   732,   149,    69,  2627,    11],\n",
              "        [ 3876,  1107,     8,  1462,     8,    12,    12,     8,   197,  8042,\n",
              "          5711,     8,     9,    15,    11,  1375, 13268,     5,    13,   111],\n",
              "        [   99,  1551,    10,    13,    10,     4,     4,    10,    48,   159,\n",
              "         19679,    10, 18736,    49,     8,    45,    12,   383,    17,    35],\n",
              "        [   53, 10674,     9,  4302,     9,   157,   356,     9,    74,    22,\n",
              "            12,     9,     7,    19,    10,    41,    36,    13,   293,  7943],\n",
              "        [    8,    31,    26,    24,     5,    35,     6,   301,    61,  8278,\n",
              "             4,  3396,     8,   686,     9,   737,   695,     4,    15,     6],\n",
              "        [   10,    33,  6443,    15,     8,  2686,     3,   440,    81,    23,\n",
              "           108,     5,    10, 10155,     5,    18,    13,  1308,     8,  1448],\n",
              "        [    9,  8440,    12,   323,    10,   397,    14,  5328,    12,   473,\n",
              "             6,    38,     9,   347,    44,  1142,  4145,    20,    10,   158],\n",
              "        [ 3896,     5,  6129,     6,     9,    13,     4,    26,    27,    18,\n",
              "         25714,  3625,    28,     6,    63,    93,   195,     4,     9,    39],\n",
              "        [   28,    33,    56,  1417,     5,    94,  6125,     4,     5,    31,\n",
              "             5,    32,    12,     4,  1544,    87,    29,  2394,   866,  5680],\n",
              "        [  790,  2725,  9474,    35,     8,   771,  5315,   910,     4,    17,\n",
              "            54,     4,     4,  4483,  1009,  2462,  3257,    11,    17,    85],\n",
              "        [   53,     5,     6,   851,    10,    11,    13,    24,  3664, 28803,\n",
              "          4185,   455,   450,   231,    12,    11,    25,  7081,   189,    33],\n",
              "        [28869,     4,    12,    13,     9,  3505,   347,   948, 12089,    17,\n",
              "          6438,  3693,     8,    15,  1637,  1287,  3680,     6,   184,  2261],\n",
              "        [    5,  1560,  9091,  1411,     5,  2168,    93,     5,  9115,     5,\n",
              "             5,   183,    10,   522,    72,     6,     5,    85,   572,  2660],\n",
              "        [ 6217,    11,     5,    42,     8,     5,     4,   768,    31,    15,\n",
              "          1927,     5,     9,    48,  1075,     3,    62,   179,    35,    11],\n",
              "        [    6,     4,    30,   705,    10,   441,  4073,    11,  8933,   283,\n",
              "             4,    25,  1160,   648,     7,    14,    29,    61,    13,   319],\n",
              "        [ 3876,  2333,   135,     7,     9, 11361,    11,  1936,  9829,    38,\n",
              "           649,     8,    27,   423,  6996,     3,  6665,   352,  2137,   231],\n",
              "        [    7,     7,     4,   211,     5,    20,   231,    45,  6214,   158,\n",
              "          1057,    10,    12,    12,   189,    14,    26,   187,   405, 14028],\n",
              "        [    4,   517,   362,  6168,     8,  1022,    15,   327,  3112,  3929,\n",
              "             7,     9,     4,     4,     4,    16,    33,     7,     7,  1993],\n",
              "        [ 5054,    66,  2429,    56,    10,  3128,   517,  1519,   220,    13,\n",
              "             4,  8857,  1891,   564,  2120,    16,   289,   281,     4,  3829],\n",
              "        [   99, 16509,     7,  3476,     9,  1957,    48,    58,     4,   599,\n",
              "         11019,    21,   141,    20,    18,    16,     8,  4158,   789,     6],\n",
              "        [   27,  1072,  4038, 10060,     5,    11,   169,    15, 11115, 12527,\n",
              "             7,  3396,     6,    15,    47,  1415,    10,   166,   332,    68],\n",
              "        [    5,     6,   362,    22,  7991,  7995,   423,  3207,     6,     5,\n",
              "             4,     6,   116,   419,  2843,  3435,     9,    31,     7,     8]],\n",
              "       device=&#x27;cuda:0&#x27;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel forward ninp                                                               </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>200            </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel forward encoder                                                            </td><td>               </td></tr>\n",
              "<tr><td>self.encoder = nn.Embedding(ntoken, ninp)                                                   </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 200]  </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 0.6296,  1.2056,  0.5395,  ..., -0.5701,  0.7327,  0.3456],\n",
              "         [-1.0710,  0.4986, -0.7773,  ...,  0.7432, -0.9552, -1.1726],\n",
              "         [ 0.5623, -0.2013, -0.3430,  ..., -0.3672, -1.0387,  1.0420],\n",
              "         ...,\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196],\n",
              "         [ 0.8175, -1.3027,  0.2762,  ...,  1.3834,  0.6806, -0.2470],\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196]],\n",
              "\n",
              "        [[-0.3777, -0.7488,  1.2727,  ..., -1.2898,  1.2599, -0.6048],\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196],\n",
              "         [ 1.3995,  1.2656, -0.5974,  ...,  0.0226,  0.5102, -0.9966],\n",
              "         ...,\n",
              "         [ 0.2546,  0.1175, -1.2223,  ...,  0.1262,  1.3427, -0.4126],\n",
              "         [-0.3865, -1.1064,  0.9696,  ...,  0.7391,  0.4150,  0.6714],\n",
              "         [ 0.8175, -1.3027,  0.2762,  ...,  1.3834,  0.6806, -0.2470]],\n",
              "\n",
              "        [[ 0.6296,  1.2056,  0.5395,  ..., -0.5701,  0.7327,  0.3456],\n",
              "         [-1.1282, -1.0816,  1.0877,  ..., -1.0312, -0.7497,  0.0652],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         ...,\n",
              "         [ 1.0889,  1.3122,  0.7571,  ...,  0.7597, -1.2613, -0.1293],\n",
              "         [ 1.3556,  0.4301,  0.7566,  ...,  0.4066, -0.8985,  0.0773],\n",
              "         [-1.1495, -1.2049,  0.2490,  ..., -0.5203,  1.2111, -0.6533]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.4653, -0.5762,  0.7049,  ..., -0.0536,  0.3482,  1.3959],\n",
              "         [ 1.3189,  1.4018, -0.5511,  ..., -1.0950,  1.1791,  0.9685],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         ...,\n",
              "         [-0.7836, -0.0401,  0.3524,  ...,  0.4935,  0.9947, -0.6621],\n",
              "         [-0.3705,  0.1265,  0.2832,  ...,  0.8935,  0.3942,  0.9780],\n",
              "         [ 0.2207, -0.5117,  0.4004,  ...,  1.3058,  0.8900, -1.2803]],\n",
              "\n",
              "        [[-1.0474, -1.0763,  0.8594,  ..., -0.1829,  1.1326,  0.1786],\n",
              "         [ 0.2676,  0.6881, -0.4718,  ..., -0.5624,  0.6407,  0.8005],\n",
              "         [ 1.3648, -0.3447,  1.1287,  ..., -1.2275, -0.9367, -0.9238],\n",
              "         ...,\n",
              "         [ 1.2534, -1.1667, -0.3783,  ...,  1.1653, -1.2677, -1.0537],\n",
              "         [-0.7944,  0.3896,  0.0235,  ...,  0.0447,  0.0735, -0.7180],\n",
              "         [-1.0671, -0.8423, -0.1596,  ..., -0.0432, -0.2677,  1.1207]],\n",
              "\n",
              "        [[-0.8063,  0.9041, -0.9540,  ...,  0.2503,  0.5757,  0.1270],\n",
              "         [ 0.2207, -0.5117,  0.4004,  ...,  1.3058,  0.8900, -1.2803],\n",
              "         [ 1.3365,  0.4551,  0.6218,  ...,  0.0781,  0.2147,  0.5753],\n",
              "         ...,\n",
              "         [-0.5697,  0.5471,  0.8227,  ..., -0.6340, -1.1329, -0.7849],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         [-0.6097,  1.1997, -0.7492,  ..., -0.0157,  0.6240, -1.3830]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;MulBackward0&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>PositionalEncoding  forward                                                                 </td><td>               </td></tr>\n",
              "<tr><td>def forward(self, x):                                                                       </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 200]  </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 0.6296,  1.2056,  0.5395,  ..., -0.5701,  0.7327,  0.3456],\n",
              "         [-1.0710,  0.4986, -0.7773,  ...,  0.7432, -0.9552, -1.1726],\n",
              "         [ 0.5623, -0.2013, -0.3430,  ..., -0.3672, -1.0387,  1.0420],\n",
              "         ...,\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196],\n",
              "         [ 0.8175, -1.3027,  0.2762,  ...,  1.3834,  0.6806, -0.2470],\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196]],\n",
              "\n",
              "        [[-0.3777, -0.7488,  1.2727,  ..., -1.2898,  1.2599, -0.6048],\n",
              "         [-1.0169, -0.0077, -0.1658,  ...,  1.1510, -0.8406, -0.2196],\n",
              "         [ 1.3995,  1.2656, -0.5974,  ...,  0.0226,  0.5102, -0.9966],\n",
              "         ...,\n",
              "         [ 0.2546,  0.1175, -1.2223,  ...,  0.1262,  1.3427, -0.4126],\n",
              "         [-0.3865, -1.1064,  0.9696,  ...,  0.7391,  0.4150,  0.6714],\n",
              "         [ 0.8175, -1.3027,  0.2762,  ...,  1.3834,  0.6806, -0.2470]],\n",
              "\n",
              "        [[ 0.6296,  1.2056,  0.5395,  ..., -0.5701,  0.7327,  0.3456],\n",
              "         [-1.1282, -1.0816,  1.0877,  ..., -1.0312, -0.7497,  0.0652],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         ...,\n",
              "         [ 1.0889,  1.3122,  0.7571,  ...,  0.7597, -1.2613, -0.1293],\n",
              "         [ 1.3556,  0.4301,  0.7566,  ...,  0.4066, -0.8985,  0.0773],\n",
              "         [-1.1495, -1.2049,  0.2490,  ..., -0.5203,  1.2111, -0.6533]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.4653, -0.5762,  0.7049,  ..., -0.0536,  0.3482,  1.3959],\n",
              "         [ 1.3189,  1.4018, -0.5511,  ..., -1.0950,  1.1791,  0.9685],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         ...,\n",
              "         [-0.7836, -0.0401,  0.3524,  ...,  0.4935,  0.9947, -0.6621],\n",
              "         [-0.3705,  0.1265,  0.2832,  ...,  0.8935,  0.3942,  0.9780],\n",
              "         [ 0.2207, -0.5117,  0.4004,  ...,  1.3058,  0.8900, -1.2803]],\n",
              "\n",
              "        [[-1.0474, -1.0763,  0.8594,  ..., -0.1829,  1.1326,  0.1786],\n",
              "         [ 0.2676,  0.6881, -0.4718,  ..., -0.5624,  0.6407,  0.8005],\n",
              "         [ 1.3648, -0.3447,  1.1287,  ..., -1.2275, -0.9367, -0.9238],\n",
              "         ...,\n",
              "         [ 1.2534, -1.1667, -0.3783,  ...,  1.1653, -1.2677, -1.0537],\n",
              "         [-0.7944,  0.3896,  0.0235,  ...,  0.0447,  0.0735, -0.7180],\n",
              "         [-1.0671, -0.8423, -0.1596,  ..., -0.0432, -0.2677,  1.1207]],\n",
              "\n",
              "        [[-0.8063,  0.9041, -0.9540,  ...,  0.2503,  0.5757,  0.1270],\n",
              "         [ 0.2207, -0.5117,  0.4004,  ...,  1.3058,  0.8900, -1.2803],\n",
              "         [ 1.3365,  0.4551,  0.6218,  ...,  0.0781,  0.2147,  0.5753],\n",
              "         ...,\n",
              "         [-0.5697,  0.5471,  0.8227,  ..., -0.6340, -1.1329, -0.7849],\n",
              "         [-0.8763,  0.5827,  0.8291,  ...,  1.3790,  1.2180,  0.5548],\n",
              "         [-0.6097,  1.1997, -0.7492,  ..., -0.0157,  0.6240, -1.3830]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;MulBackward0&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>PositionalEncoding  forward self.pe                                                         </td><td>               </td></tr>\n",
              "<tr><td>x = x + self.pe[:x.size(0), :]                                                              </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 200]  </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 0.6296,  2.2056,  0.5395,  ...,  0.4299,  0.7327,  1.3456],\n",
              "         [-1.0710,  1.4986, -0.7773,  ...,  1.7432, -0.9552, -0.1726],\n",
              "         [ 0.5623,  0.7987, -0.3430,  ...,  0.6328, -1.0387,  2.0420],\n",
              "         ...,\n",
              "         [-1.0169,  0.9923, -0.1658,  ...,  2.1510, -0.8406,  0.7804],\n",
              "         [ 0.8175, -0.3027,  0.2762,  ...,  2.3834,  0.6806,  0.7530],\n",
              "         [-1.0169,  0.9923, -0.1658,  ...,  2.1510, -0.8406,  0.7804]],\n",
              "\n",
              "        [[ 0.4637, -0.2085,  2.0635,  ..., -0.2898,  1.2600,  0.3952],\n",
              "         [-0.1755,  0.5326,  0.6249,  ...,  2.1510, -0.8405,  0.7804],\n",
              "         [ 2.2410,  1.8059,  0.1934,  ...,  1.0226,  0.5103,  0.0034],\n",
              "         ...,\n",
              "         [ 1.0961,  0.6578, -0.4316,  ...,  1.1262,  1.3428,  0.5874],\n",
              "         [ 0.4550, -0.5661,  1.7603,  ...,  1.7391,  0.4152,  1.6714],\n",
              "         [ 1.6590, -0.7624,  1.0669,  ...,  2.3834,  0.6807,  0.7530]],\n",
              "\n",
              "        [[ 1.5389,  0.7895,  1.5076,  ...,  0.4299,  0.7329,  1.3456],\n",
              "         [-0.2189, -1.4977,  2.0558,  ..., -0.0312, -0.7495,  1.0652],\n",
              "         [ 0.0330,  0.1665,  1.7972,  ...,  2.3790,  1.2182,  1.5548],\n",
              "         ...,\n",
              "         [ 1.9982,  0.8960,  1.7252,  ...,  1.7597, -1.2611,  0.8707],\n",
              "         [ 2.2649,  0.0140,  1.7248,  ...,  1.4066, -0.8983,  1.0773],\n",
              "         [-0.2402, -1.6210,  1.2171,  ...,  0.4797,  1.2114,  0.3467]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.0167,  0.2580, -0.0846,  ...,  0.9464,  0.3517,  2.3959],\n",
              "         [ 1.8703,  2.2360, -1.3406,  ..., -0.0950,  1.1827,  1.9685],\n",
              "         [-0.3248,  1.4169,  0.0396,  ...,  2.3790,  1.2215,  1.5548],\n",
              "         ...,\n",
              "         [-0.2322,  0.7941, -0.4371,  ...,  1.4935,  0.9982,  0.3379],\n",
              "         [ 0.1809,  0.9607, -0.5063,  ...,  1.8935,  0.3977,  1.9780],\n",
              "         [ 0.7721,  0.3225, -0.3891,  ...,  2.3058,  0.8935, -0.2803]],\n",
              "\n",
              "        [[-0.0475, -1.0896, -0.1092,  ...,  0.8171,  1.1362,  1.1786],\n",
              "         [ 1.2675,  0.6748, -1.4404,  ...,  0.4376,  0.6444,  1.8005],\n",
              "         [ 2.3647, -0.3580,  0.1601,  ..., -0.2275, -0.9331,  0.0762],\n",
              "         ...,\n",
              "         [ 2.2533, -1.1800, -1.3469,  ...,  2.1653, -1.2640, -0.0537],\n",
              "         [ 0.2055,  0.3763, -0.9451,  ...,  1.0447,  0.0771,  0.2820],\n",
              "         [-0.0672, -0.8556, -1.1282,  ...,  0.9568, -0.2640,  2.1207]],\n",
              "\n",
              "        [[-0.2772,  0.0555, -1.3503,  ...,  1.2502,  0.5794,  1.1270],\n",
              "         [ 0.7498, -1.3602,  0.0041,  ...,  2.3058,  0.8937, -0.2803],\n",
              "         [ 1.8656, -0.3934,  0.2254,  ...,  1.0781,  0.2184,  1.5753],\n",
              "         ...,\n",
              "         [-0.0406, -0.3015,  0.4264,  ...,  0.3660, -1.1291,  0.2151],\n",
              "         [-0.3472, -0.2659,  0.4328,  ...,  2.3790,  1.2217,  1.5548],\n",
              "         [-0.0806,  0.3512, -1.1455,  ...,  0.9842,  0.6277, -0.3830]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddBackward0&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel forward pos_encoder                                                        </td><td>               </td></tr>\n",
              "<tr><td>src = self.pos_encoder(src)                                                                 </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 200]  </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 0.7870,  2.7570,  0.0000,  ...,  0.5374,  0.9158,  1.6820],\n",
              "         [-1.3387,  0.0000, -0.9717,  ...,  2.1790,  0.0000,  0.0000],\n",
              "         [ 0.0000,  0.0000, -0.4288,  ...,  0.7910, -1.2984,  2.5525],\n",
              "         ...,\n",
              "         [-1.2712,  1.2404, -0.2072,  ...,  2.6888, -1.0508,  0.0000],\n",
              "         [ 1.0219, -0.3784,  0.3452,  ...,  2.9793,  0.0000,  0.9413],\n",
              "         [-1.2712,  1.2404, -0.2072,  ...,  0.0000, -1.0508,  0.0000]],\n",
              "\n",
              "        [[ 0.5797, -0.2607,  2.5793,  ..., -0.3623,  1.5750,  0.4941],\n",
              "         [-0.2193,  0.6658,  0.7812,  ...,  2.6888, -1.0507,  0.9756],\n",
              "         [ 0.0000,  2.2574,  0.2417,  ...,  1.2783,  0.0000,  0.0043],\n",
              "         ...,\n",
              "         [ 1.3701,  0.0000, -0.5395,  ...,  1.4078,  1.6785,  0.7343],\n",
              "         [ 0.5687, -0.7077,  2.2004,  ...,  2.1738,  0.5189,  2.0892],\n",
              "         [ 2.0737, -0.9530,  1.3337,  ...,  0.0000,  0.8509,  0.9413]],\n",
              "\n",
              "        [[ 1.9236,  0.9868,  1.8845,  ...,  0.5374,  0.9161,  1.6820],\n",
              "         [-0.2737, -1.8721,  2.5697,  ..., -0.0390,  0.0000,  1.3315],\n",
              "         [ 0.0413,  0.0000,  0.0000,  ...,  2.9737,  0.0000,  1.9435],\n",
              "         ...,\n",
              "         [ 2.4977,  0.0000,  0.0000,  ...,  2.1996, -1.5763,  1.0883],\n",
              "         [ 2.8311,  0.0175,  2.1559,  ...,  1.7583,  0.0000,  1.3466],\n",
              "         [-0.3003,  0.0000,  1.5214,  ...,  0.5996,  1.5142,  0.4334]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.2709,  0.3225, -0.1058,  ...,  0.0000,  0.0000,  2.9948],\n",
              "         [ 0.0000,  2.7950, -1.6757,  ..., -0.1187,  1.4783,  2.4606],\n",
              "         [ 0.0000,  1.7711,  0.0495,  ...,  2.9737,  1.5269,  1.9435],\n",
              "         ...,\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  1.8669,  1.2477,  0.4223],\n",
              "         [ 0.0000,  1.2009,  0.0000,  ...,  2.3668,  0.4972,  2.4725],\n",
              "         [ 0.0000,  0.4032, -0.4863,  ...,  2.8823,  1.1169, -0.3504]],\n",
              "\n",
              "        [[ 0.0000,  0.0000, -0.1366,  ...,  1.0214,  1.4203,  1.4733],\n",
              "         [ 1.5844,  0.0000, -1.8005,  ...,  0.5470,  0.8054,  2.2506],\n",
              "         [ 0.0000,  0.0000,  0.2002,  ...,  0.0000, -1.1663,  0.0953],\n",
              "         ...,\n",
              "         [ 0.0000, -1.4750, -1.6837,  ...,  2.7067, -1.5801, -0.0671],\n",
              "         [ 0.2568,  0.4704, -1.1814,  ...,  1.3059,  0.0964,  0.3525],\n",
              "         [-0.0840, -1.0695, -1.4103,  ...,  1.1960,  0.0000,  2.6509]],\n",
              "\n",
              "        [[-0.3465,  0.0694, -1.6879,  ...,  1.5628,  0.7243,  1.4088],\n",
              "         [ 0.0000, -1.7003,  0.0051,  ...,  2.8823,  1.1172, -0.3504],\n",
              "         [ 2.3319, -0.4918,  0.2818,  ...,  1.3476,  0.2731,  1.9692],\n",
              "         ...,\n",
              "         [-0.0508, -0.3769,  0.5330,  ...,  0.0000, -1.4114,  0.2689],\n",
              "         [ 0.0000, -0.3324,  0.5410,  ...,  2.9737,  1.5272,  1.9435],\n",
              "         [ 0.0000,  0.4390, -1.4319,  ...,  1.2303,  0.7847, -0.4788]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;FusedDropoutBackward&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel transformer_encoder output                                                 </td><td>               </td></tr>\n",
              "<tr><td>output = self.transformer_encoder(src, self.src_mask)                                       </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 200]  </td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[ 1.1672,  1.6414, -0.1480,  ...,  0.0601,  0.0182, -0.0478],\n",
              "         [-0.3979, -0.2393, -0.4534,  ...,  0.9453, -1.3734, -1.1942],\n",
              "         [ 0.5709, -0.7618,  0.3959,  ..., -1.5867, -1.9864,  1.7826],\n",
              "         ...,\n",
              "         [-0.1417,  0.5740,  0.2775,  ...,  0.1606, -0.9231, -0.9392],\n",
              "         [ 0.4671, -0.5679, -0.3173,  ...,  1.7328, -0.2877, -1.7941],\n",
              "         [-0.3674,  0.9563,  0.3769,  ..., -1.9327, -0.7485, -1.1340]],\n",
              "\n",
              "        [[ 1.3190,  0.2270,  1.0983,  ..., -0.3307,  0.1160, -0.4210],\n",
              "         [-0.0610,  0.4592,  0.2425,  ...,  1.6191, -1.5594,  0.5235],\n",
              "         [-0.1085,  0.9928,  0.0685,  ..., -1.5343, -1.4593, -0.9181],\n",
              "         ...,\n",
              "         [ 1.8768, -0.2129,  0.9498,  ..., -0.7608,  0.1464,  0.0516],\n",
              "         [-0.7131,  0.4470,  1.2445,  ...,  1.1318, -0.8088, -0.2519],\n",
              "         [ 2.5171, -1.1945,  1.8089,  ..., -1.1117,  0.1120, -0.3211]],\n",
              "\n",
              "        [[ 1.4534,  0.5538,  0.7929,  ...,  0.3941,  0.0108,  0.9685],\n",
              "         [ 0.3217, -1.3246,  1.6060,  ..., -0.9673, -1.3253,  1.7209],\n",
              "         [-0.0496, -0.8169,  0.0703,  ...,  0.2488, -1.4639,  1.0465],\n",
              "         ...,\n",
              "         [ 0.8224, -0.0921,  0.0695,  ..., -0.0207, -1.6421, -0.3948],\n",
              "         [ 1.4681, -0.5121,  1.1212,  ...,  0.4345, -0.4021, -0.5554],\n",
              "         [-0.7680, -0.4122,  0.7649,  ..., -0.6053,  1.3754, -0.6858]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 1.0482, -0.2793,  0.0670,  ..., -0.0921, -0.2098,  1.0644],\n",
              "         [-0.0128,  1.2481, -1.1978,  ..., -0.3291,  0.2318,  0.4379],\n",
              "         [-0.3643,  1.1038, -0.1541,  ...,  1.2070,  0.5601,  0.4866],\n",
              "         ...,\n",
              "         [-0.0560, -0.0760, -0.4982,  ...,  0.2642, -0.1311, -0.2111],\n",
              "         [-0.1738,  0.5396, -1.3336,  ...,  1.6071,  0.1669,  1.2434],\n",
              "         [-0.2012, -0.7146,  0.0646,  ...,  1.5929,  0.3657, -1.4621]],\n",
              "\n",
              "        [[-0.0996, -0.3889,  0.0963,  ..., -0.2265,  0.7862,  0.3424],\n",
              "         [ 1.4361,  0.1722, -1.0460,  ..., -0.0964,  0.1876,  1.4119],\n",
              "         [-0.2591, -0.6182,  0.4220,  ..., -0.6375, -1.5359, -0.4409],\n",
              "         ...,\n",
              "         [ 0.1614, -1.3309, -1.3903,  ...,  1.6391, -1.8631, -0.1727],\n",
              "         [-0.1469,  0.1747, -1.7204,  ...,  0.5534, -1.0004, -0.4600],\n",
              "         [-0.6853, -1.3908, -0.6247,  ..., -0.0553, -1.3396,  1.3719]],\n",
              "\n",
              "        [[-0.7091, -0.6342, -1.5536,  ...,  0.5214,  0.0274,  0.3564],\n",
              "         [ 0.4399, -1.8885,  0.0342,  ...,  1.2658, -0.0159, -1.7664],\n",
              "         [ 1.5336, -0.4241, -0.2298,  ...,  0.0505, -0.8964,  0.8160],\n",
              "         ...,\n",
              "         [-0.0103, -0.2915, -0.6925,  ..., -0.8752, -2.7850, -0.5801],\n",
              "         [-1.2665, -0.5650,  0.3226,  ...,  2.0099,  0.9635,  0.1025],\n",
              "         [-0.0928,  0.2756, -1.2986,  ...,  0.5087,  0.7686, -1.3510]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;NativeLayerNormBackward&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>TransformerModel decoder output                                                             </td><td>               </td></tr>\n",
              "<tr><td>output = self.decoder(output)                                                               </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 28871]</td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[-0.5603, -0.2689,  0.6781,  ...,  1.1186, -1.1365,  0.9792],\n",
              "         [ 0.7297,  0.0042,  0.7295,  ...,  1.4944,  0.3426,  0.3558],\n",
              "         [-1.5441,  0.3779, -0.7228,  ..., -0.2812,  0.2051,  1.9949],\n",
              "         ...,\n",
              "         [-0.9502, -0.7102, -1.5954,  ..., -0.0873, -0.3256,  0.7836],\n",
              "         [-0.2954, -0.0997,  0.0433,  ...,  0.1274, -0.1398,  0.4580],\n",
              "         [-0.4218, -1.1341, -0.2441,  ...,  0.1567, -0.0283,  1.3187]],\n",
              "\n",
              "        [[ 0.6307, -0.6576, -0.3077,  ...,  0.9479, -0.2476,  0.6541],\n",
              "         [-0.4380, -0.7274, -0.5303,  ...,  0.7550,  0.3078, -0.0046],\n",
              "         [-0.1940, -0.3214, -1.0238,  ...,  1.2127, -0.2604,  0.5708],\n",
              "         ...,\n",
              "         [ 0.1912, -0.0528, -0.1935,  ...,  0.1144, -0.8419,  1.1859],\n",
              "         [ 0.3331,  0.3042, -0.3755,  ..., -0.7556, -0.1380,  0.2637],\n",
              "         [ 0.3948, -1.0708, -0.3639,  ...,  0.2003, -1.1100,  0.6533]],\n",
              "\n",
              "        [[-0.3534,  0.5059,  0.6559,  ...,  0.0615, -1.0441,  0.6394],\n",
              "         [-0.8082, -1.2675, -0.5089,  ...,  0.7261, -1.3570,  0.0809],\n",
              "         [-0.8772, -0.0221,  0.1568,  ..., -0.1718, -0.1537,  0.7465],\n",
              "         ...,\n",
              "         [-0.9814, -0.7042, -1.2010,  ...,  1.1830, -0.3362,  0.9094],\n",
              "         [ 0.2401, -0.4299, -0.1277,  ...,  0.2967, -0.1143, -0.1432],\n",
              "         [-0.7992, -0.7146, -0.5449,  ..., -0.7768, -0.2067, -0.8876]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.7914,  0.1163, -0.1528,  ...,  0.0400,  0.7905, -0.1057],\n",
              "         [-0.5888, -0.8007,  1.1208,  ..., -0.1888, -0.8212,  0.5874],\n",
              "         [ 0.1592,  0.1216,  1.3573,  ..., -0.6937, -0.5318, -0.3518],\n",
              "         ...,\n",
              "         [ 0.0487, -0.6947, -1.6190,  ...,  1.1880, -0.9407,  1.6598],\n",
              "         [-0.3371, -1.0688, -0.8720,  ...,  0.2490, -0.1156, -0.4401],\n",
              "         [-0.1740, -0.1131, -0.2878,  ..., -0.2560, -0.2232,  0.4674]],\n",
              "\n",
              "        [[-1.0071, -0.1671, -0.4191,  ..., -0.6921, -0.3978,  1.3268],\n",
              "         [-0.8798, -0.9711, -0.2297,  ...,  0.6235, -1.6562, -0.0972],\n",
              "         [-0.2432,  0.2633, -0.6117,  ...,  0.2672, -1.0764,  0.3401],\n",
              "         ...,\n",
              "         [ 0.1381,  1.0071, -0.9556,  ..., -0.6508, -0.8826,  0.3873],\n",
              "         [-0.6372, -1.1045,  0.4602,  ...,  0.3702,  0.1615,  0.7241],\n",
              "         [ 0.1112, -0.1475, -0.5710,  ..., -0.8747, -0.8276,  1.2599]],\n",
              "\n",
              "        [[-0.5385, -0.3039, -0.6497,  ...,  1.2670,  0.2429,  0.8858],\n",
              "         [-0.0077, -0.2838, -0.7262,  ..., -0.1741, -0.4768,  0.6201],\n",
              "         [ 0.3991,  0.5370, -1.1123,  ..., -1.0315, -1.6222,  0.8126],\n",
              "         ...,\n",
              "         [-0.3345, -0.9915, -1.2652,  ...,  0.5158, -0.2149,  0.3909],\n",
              "         [-0.3734, -0.2374,  0.3885,  ..., -0.9347, -0.2840, -0.5503],\n",
              "         [-0.3004, -0.0411, -0.0149,  ...,  0.5085, -0.9034, -0.7745]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddBackward0&gt;)                </td></tr>\n",
              "<tr><td>-----                                                                                       </td><td>-----          </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>train output                                                                                </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>               </td></tr>\n",
              "<tr><td>                                                                                            </td><td>[35, 20, 28871]</td></tr>\n",
              "<tr><td>                                                                                            </td><td>tensor([[[-0.5603, -0.2689,  0.6781,  ...,  1.1186, -1.1365,  0.9792],\n",
              "         [ 0.7297,  0.0042,  0.7295,  ...,  1.4944,  0.3426,  0.3558],\n",
              "         [-1.5441,  0.3779, -0.7228,  ..., -0.2812,  0.2051,  1.9949],\n",
              "         ...,\n",
              "         [-0.9502, -0.7102, -1.5954,  ..., -0.0873, -0.3256,  0.7836],\n",
              "         [-0.2954, -0.0997,  0.0433,  ...,  0.1274, -0.1398,  0.4580],\n",
              "         [-0.4218, -1.1341, -0.2441,  ...,  0.1567, -0.0283,  1.3187]],\n",
              "\n",
              "        [[ 0.6307, -0.6576, -0.3077,  ...,  0.9479, -0.2476,  0.6541],\n",
              "         [-0.4380, -0.7274, -0.5303,  ...,  0.7550,  0.3078, -0.0046],\n",
              "         [-0.1940, -0.3214, -1.0238,  ...,  1.2127, -0.2604,  0.5708],\n",
              "         ...,\n",
              "         [ 0.1912, -0.0528, -0.1935,  ...,  0.1144, -0.8419,  1.1859],\n",
              "         [ 0.3331,  0.3042, -0.3755,  ..., -0.7556, -0.1380,  0.2637],\n",
              "         [ 0.3948, -1.0708, -0.3639,  ...,  0.2003, -1.1100,  0.6533]],\n",
              "\n",
              "        [[-0.3534,  0.5059,  0.6559,  ...,  0.0615, -1.0441,  0.6394],\n",
              "         [-0.8082, -1.2675, -0.5089,  ...,  0.7261, -1.3570,  0.0809],\n",
              "         [-0.8772, -0.0221,  0.1568,  ..., -0.1718, -0.1537,  0.7465],\n",
              "         ...,\n",
              "         [-0.9814, -0.7042, -1.2010,  ...,  1.1830, -0.3362,  0.9094],\n",
              "         [ 0.2401, -0.4299, -0.1277,  ...,  0.2967, -0.1143, -0.1432],\n",
              "         [-0.7992, -0.7146, -0.5449,  ..., -0.7768, -0.2067, -0.8876]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.7914,  0.1163, -0.1528,  ...,  0.0400,  0.7905, -0.1057],\n",
              "         [-0.5888, -0.8007,  1.1208,  ..., -0.1888, -0.8212,  0.5874],\n",
              "         [ 0.1592,  0.1216,  1.3573,  ..., -0.6937, -0.5318, -0.3518],\n",
              "         ...,\n",
              "         [ 0.0487, -0.6947, -1.6190,  ...,  1.1880, -0.9407,  1.6598],\n",
              "         [-0.3371, -1.0688, -0.8720,  ...,  0.2490, -0.1156, -0.4401],\n",
              "         [-0.1740, -0.1131, -0.2878,  ..., -0.2560, -0.2232,  0.4674]],\n",
              "\n",
              "        [[-1.0071, -0.1671, -0.4191,  ..., -0.6921, -0.3978,  1.3268],\n",
              "         [-0.8798, -0.9711, -0.2297,  ...,  0.6235, -1.6562, -0.0972],\n",
              "         [-0.2432,  0.2633, -0.6117,  ...,  0.2672, -1.0764,  0.3401],\n",
              "         ...,\n",
              "         [ 0.1381,  1.0071, -0.9556,  ..., -0.6508, -0.8826,  0.3873],\n",
              "         [-0.6372, -1.1045,  0.4602,  ...,  0.3702,  0.1615,  0.7241],\n",
              "         [ 0.1112, -0.1475, -0.5710,  ..., -0.8747, -0.8276,  1.2599]],\n",
              "\n",
              "        [[-0.5385, -0.3039, -0.6497,  ...,  1.2670,  0.2429,  0.8858],\n",
              "         [-0.0077, -0.2838, -0.7262,  ..., -0.1741, -0.4768,  0.6201],\n",
              "         [ 0.3991,  0.5370, -1.1123,  ..., -1.0315, -1.6222,  0.8126],\n",
              "         ...,\n",
              "         [-0.3345, -0.9915, -1.2652,  ...,  0.5158, -0.2149,  0.3909],\n",
              "         [-0.3734, -0.2374,  0.3885,  ..., -0.9347, -0.2840, -0.5503],\n",
              "         [-0.3004, -0.0411, -0.0149,  ...,  0.5085, -0.9034, -0.7745]]],\n",
              "       device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddBackward0&gt;)                </td></tr>\n",
              "</tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxUDGc-OtCNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXoJBfrAusiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAeP_qBc9dEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0n6uWCN9d3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5d84495c-3ca8-4ded-bf26-96c52c897f48"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.00 | test ppl   148.34\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPXIE7dfT1s_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}